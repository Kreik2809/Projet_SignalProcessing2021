% % % % % % %
%  Packages %
% % % % % % %

%---Base packages
\documentclass[a4paper,12pt]{report}	% document type (article, report, book)
\usepackage[utf8]{inputenc}			% encoding
\usepackage[T1]{fontenc}			% accent
\usepackage{lmodern}				% latin font
\usepackage{appendix}				% to be able to use appendices
\usepackage{float,graphicx}	
\usepackage{algorithm}
\usepackage{algorithmic}	

%---Language(s)
\usepackage[english,frenchb]{babel}	% last language = typography by default
\addto\captionsfrench{				% to change the french names of...
	\renewcommand{\appendixpagename}{Annexes}	% (default Appendices)
	\renewcommand{\appendixtocname}{Annexes}	% (default Appendices)
	\renewcommand{\tablename}{\textsc{Tableau}}	% (default \textsc{Table})
}

%---Page layout
%------> margins
	% 1st option -> geometry package
		%\usepackage[a4paper]{geometry}		% default parameters for A4
		%\usepackage[top=2in, bottom=1.5in, left=1in, right=1in]{geometry}
	% 2nd option -> a4wide package
		\usepackage{a4wide}		% A4 with smaller margins (the one I've chosen)
	% 3rd option -> fullpage package
		%\usepackage{fullpage}
%------> chapter style
	% 1st option -> fncychap package
		%\usepackage[style]{fncychap}		% style = Lenny, Bjornstrup, Sonny, Conny
	% 2nd option -> customized styles
		%
%------> cover page (UMONS template)
	\usepackage[fs]{umons-coverpage}		% NEED "umons-coverpage.sty" file
	\umonsAuthor{Réalisé par Louis \textsc{Dascotte} \\ \& Nicolas \textsc{Delplanque} \\ \& Nicolas \textsc{Sournac} } 
	\umonsTitle{Traitement du signal : Projet}
	\umonsSubtitle{Speaker classification}
	\umonsDocumentType{I-ISIA-030}
	\umonsDate{3e Bachelier en Sciences Informatiques\\ Année 2021-2022}

%---Numbering
\setcounter{secnumdepth}{2}			% numerotation depth (1=sec and all above)
\setcounter{tocdepth}{2}			% table of contents depth (1=sec and above)

%---Mathematics
\usepackage{amsmath}				% base package for mathematics
\usepackage{amsfonts}				% base package for mathematics
\usepackage{amssymb}				% base package for mathematics
%\usepackage{amsthm}				% theorem and proof environments
%\usepackage{cases}					% numcases environment
%\usepackage{mathrsfs}				% \mathscf command ('L' of Laplace-Transform,...)

%---Floating objects (images, tables,...)
\usepackage{float}					% better management of floating objects
\usepackage{array}					% better management of tables
\usepackage{graphicx}				% to include external images
\graphicspath{{Images/}}			% to put images in an 'Images' folder 
%\usepackage{caption}				% /!\ has priority on "memoir" class
%\usepackage{subcaption}			% subfigure and subtable environments
%\usepackage{subfig}				% \subfloat command
%\usepackage{wrapfig}				% wrapfigure environment
%\usepackage[update]{epstopdf}		% to use '.eps' files with PDFLaTeX

\setlength{\extrarowheight}{.5ex}


%---Units from International System
\usepackage{siunitx}				% \SI{}{} command (units with good typography)
\DeclareSIUnit\baud{baud}			% definition of the "baud" unit
\DeclareSIUnit\bit{bit}				% definition of the "bit" unit

%---Drawing
%\usepackage{tikz}					% useful package for drawing
%\usepackage[european]{circuitikz} 	% to draw electrical circuits

%---Bibliography
\usepackage{url}					% to encore url
\usepackage[style=numeric-comp,backend=bibtex]{biblatex}
\usepackage{csquotes}				% inverted commas in references
%\bibliography{bibli}				% your .bib file

%---"hyperref" package				% /!\ it must be the last package
\usepackage[hidelinks]{hyperref}	% clickable links (table of contents,...)


% % % % % % %
% Document	%
% % % % % % %

\begin{document}

\umonsCoverPage		% produce the cover page with UMONS and your Faculty logo
	
\pagenumbering{roman}	% if you don't use the class "book"

\begin{abstract}
Ce rapport contient l'ensemble des résultats obtenus, leurs interprétations ainsi que les explications du fonctionnement de notre implémentation du projet de traitement du signal. Ce projet consiste en la réalisation de solutions visant à classifier des personnes en fonction de leur genre à partir d'enregistrement de leur voix.
\end{abstract}

\clearpage		
\tableofcontents

\clearpage		
\pagenumbering{arabic}

{\section*{1. Exécution du code}}
\addcontentsline{toc}{chapter}{1. Exécution du code/ Structure}
{\subsection*{1.1 Librairies utilisées}}
\addcontentsline{toc}{section}{1.1 Librairies utilisées}
{\subsection*{1.2 Structure du code source}}
\addcontentsline{toc}{section}{1.2 Structure du code source}

{\section*{2. Caractéristiques étudiées}}
	\addcontentsline{toc}{chapter}{2. Caractéristiques étudiées}
Pour la réalisation du projet, nous avons été amenés à étudier différentes caractéristiques des signaux de parole en vue de les utiliser afin de pouvoir classifier les différents speakers. Pour chacune de ces caractéristiques, nous avons donc implémenté un algorithme qui permet de la calculer et nous avons ensuite analysé et interprété les résultats afin d'en déduire des règles sur lesquelles la classification allait se baser.

{\subsection*{2.1 Energie du signal}}
\addcontentsline{toc}{section}{2.1 Energie du signal}
L'énergie du signal est calculée à partir de la fonction compute\_energy() qui prend un signal en paramètre. Cette fonctionne parcourt simplement l'entièreté du signal en question et y applique la formule du calcul de l'énergie. Nous utilisons les résultats du calcule de l'énergie du signal afin de déterminer si une frame est voiced ou unvoiced.
{\subsection*{2.2 Fréquence fondamentale}}
\addcontentsline{toc}{section}{2.2 Fréquence fondamentale}
Le pitch ou la fréquence fondamentale représente la plus basse fréquence qui constitue un signal. Cette valeur doit varier entre 60 Hz pour les voix les plus graves et 550 Hz pour les voix les plus aigües. \\
Lors du projet, nous avons été amenés à calculer la fréquence fondamentale des speakers en utilisant deux méthodes différentes : La méthode basée sur l'autocorrelation et la méthode basée sur les cepstrums. Pour ce faire, nous avons créé les fonctions autocorrelation\_pitch\_estim() et cepstrum\_pitch\_estim(). \\
Pour la méthode basée sur l'autocorrelation, l'algorithme doit recevoir en paramètre une liste de minimum 5 fichiers audio. Nous lisons chaque fichier, normalisons son signal, le divisons en frames de 50 ms et récupérons uniquement les frames dont l'énergie est supérieure au seuil que nous avons déterminé graphiquement. Ensuite, pour chacune des frames restantes, nous calculons sa corrélations avec un maxlags qui vaut 200. Grâce à la fonction find\_peaks() fournie par la librairie numpy, nous récupérons facilement l'ensemble des pics du signal corrélé. Il ne nous reste plus qu'à calculer la distance en Hz entre les deux plus hauts pics pour obtenir la fréquence fondamentale de la frame. La fonction autocorrelation\_pitch\_estim() retourne la moyenne de toutes les fréquences fondamentales calculées à partir des frames en retirant les fréquences supérieures à 550 Hz considérées comme erronées. Nous considérons ce résultat étant comme la fréquence fondamentale de la personne à l'origine des fichiers audio. \\
Pour la méthode basée sur les cepstrums, le début de l'algorithme est similaire au précédent. Nous lisons également chaque fichiers audio fournis, les normalisons, les divisons en frames de 50 ms et récupérons uniquement les frames dont l'énergie est supérieur au seuil que nous avons déterminé graphiquement. Ensuite, pour chacune des frames restantes, nous calculons dans un premier temps son cepstrum de manière brut et dans un deuxième temps son cepstrum après lui avoir appliqué une fenêtre de hamming. Il suffit ensuite de trouver l'indice du plus haut pics présents dans ces deux cepstrums dans l'interval allant de 60Hz à 550Hz et de le convertir en Hz pour obtenir la fréquence fondamentale de la frame. La fonction cepstrum\_pitch\_estim() retourne la moyenne de toutes les fréquences fondamentales calculées à partir des frames. Nous considérons ce résultat comme étant la fréquence fondamentale de la personne à l'origine des fichiers audio.\\
\\
Afin de déterminer des règles de différenciations sur base de la fréquence fondamentale, nous avons exécuté 15 fois les deux algorithmes en donnant des fichiers des deux speakers. Pour BDL nous avons pris la valeur maximum obtenues sur les 15 itérations et l'avons considérées comme borne supérieure et pour SLT nous avons pris la valeur minimum obtenues sur les 15 itérations et l'avons considérées comme borne inférieure. Cela nous a donné les règles suivantes : \\
Pour la méthode basée sur l'autocorrelation, si f0 < 145Hz alors c'est BDL et si f0 > 170Hz alors c'est SLT. \\
Pour la méthode basée sur les ceptrums, si f0 < 166Hz alors c'est BDL et si f0 > 217Hz, alors c'est SLT. 

{\subsection*{2.3 Formants}}
\addcontentsline{toc}{section}{2.3 Formants}
Les formants sont les pics observés sur l'enveloppe de la réponse en fréquence. Dans le cadre de ce projet, l'estimation des formants se base sur un algorithme LPC. Pour ce faire, nous avons créé la fonction compute\_formant() qui prend un paramètre un fichier audio. L'algorithme va lire le fichier, le normaliser, et le diviser en frames de 25 ms. Pour chacune des frames, un filtre passe-haut du premier ordre lui est appliqué suivi d'une fenêtre de hamming. Nous calculons ensuite les coefficients LPC et trouvons ensuite les racines associées. Les formants sont directement déduits de ces racines complexes. L'algorithmes retourne donc une liste de formants correspondants aux formants de chacune des frames du fichier audio. 
TODO expliquer comment on a déterminer les règles de classification (voir figure)

{\subsection*{2.4 MFCCs}}
\addcontentsline{toc}{section}{2.4 MFCCs}
	
{\section*{3. Systèmes basés sur des règles}}
	\addcontentsline{toc}{chapter}{3. Systèmes basés sur des règles}
	Bien expliquer les différents systèmes + comment on les utilise + comment on a fait pour estimer leur précisions + quelles données on a utiliser pour les tester

{\subsection*{3.1 Système 01}}
\addcontentsline{toc}{section}{3.1 Système 01}

{\subsection*{3.2 Système 02}}
\addcontentsline{toc}{section}{3.2 Système 02}

{\subsection*{3.3 Système 03}}
\addcontentsline{toc}{section}{3.3 Système 03}

{\section*{4. Machine learning}}
\addcontentsline{toc}{chapter}{4. Machine learning}

\end{document}


